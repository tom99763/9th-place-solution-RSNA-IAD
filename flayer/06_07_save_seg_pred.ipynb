{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fef37e-8f27-411b-a844-7b31c04c7f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from torch import amp\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose, MapTransform, EnsureTyped, ToTensord,\n",
    "    RandFlipd, RandRotate90d, RandGaussianNoised,\n",
    "    RandCropByPosNegLabeld,SpatialPadd\n",
    ")\n",
    "import pydicom\n",
    "# from monai.data import Dataset, DataLoader\n",
    "from monai.networks.nets import UNet,DynUNet\n",
    "# from monai.losses import DiceFocalLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "# from monai.metrics import DiceMetric\n",
    "# from monai.utils import set_determinism\n",
    "from tqdm.auto import tqdm\n",
    "import cc3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986429a-2858-4cb6-a7af-d1b1e4baccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/ssd3/rsna_2025_flayer/data/series\"\n",
    "save_dir=\"/ssd3/rsna_2025_flayer/input/seg_pred\"\n",
    "device =\"cuda:2\"\n",
    "model_pth=\"model/job13_best.pt\"\n",
    "\n",
    "target_shape=(64,448,448)\n",
    "\n",
    "pred_resize_dir = \"/ssd3/rsna_2025_flayer/input/seg_pred_448_s64\"\n",
    "axis_df=pd.read_csv(\"/ssd3/rsna_2025_flayer/input/axis_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a121eda-4e20-4ce5-9d1d-1f9d18057d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strides = [\n",
    "    (1, 1, 1),   # level 0 (no downsample)\n",
    "    (2, 2, 2),   # level 1\n",
    "    (2, 2, 2),   # level 2\n",
    "    (2, 2, 2),   # level 3\n",
    "    (2, 2, 2),   # level 4\n",
    "]\n",
    "kernel_size = [(3,3,3)] * len(strides)\n",
    "upsample_kernel_size = strides[1:]\n",
    "\n",
    "filters = [32, 64, 128, 256, 320]   # nnU-Net 風格；你有 96GB，這組很穩\n",
    "\n",
    "model = DynUNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=kernel_size,\n",
    "    strides=strides,\n",
    "    upsample_kernel_size=upsample_kernel_size,\n",
    "    filters=filters,\n",
    "    dropout=0.0,\n",
    "    deep_supervision=True,   # 啟用深度監督\n",
    "    deep_supr_num=3,         # 最高層之外再用 3 個低解析輸出做監督\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.load(model_pth, map_location=device, weights_only=True)\n",
    "model.load_state_dict(ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad9b3d9-e1f7-4c97-ad2a-aa3a95b44f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4fcbb5a-aaa3-47e8-9a31-ed2a5ca5b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI_SIZE    = (96,256,256) \n",
    "val_tf = Compose([\n",
    "    SpatialPadd(\n",
    "        keys=[\"image\"], \n",
    "        spatial_size=ROI_SIZE,\n",
    "        method=\"symmetric\", \n",
    "        mode=\"constant\", \n",
    "        constant_values=0\n",
    "    ),\n",
    "    EnsureTyped(keys=[\"image\"]),\n",
    "    ToTensord(keys=[\"image\"]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7646919b-ec47-49a1-a296-7fd4ca81ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sid = valid_df[\"SeriesInstanceUID\"].values[5]\n",
    "#sid=\"1.2.826.0.1.3680043.8.498.10012790035410518400400834395242853657\"\n",
    "\n",
    "\n",
    "def get_volume(sid):\n",
    "    files = glob.glob(f\"{base_dir}/{sid}/*.dcm\")\n",
    "    slices = [pydicom.dcmread(f) for f in files]\n",
    "    \n",
    "    \n",
    "    if len(slices)==1:\n",
    "        ds=slices[0]\n",
    "        posz_list=[]\n",
    "        for frame in ds.PerFrameFunctionalGroupsSequence:\n",
    "            # Plane position sequence\n",
    "            pos = frame.PlanePositionSequence[0].ImagePositionPatient\n",
    "            posz_list.append(float(pos[2]))       \n",
    "        dy,dx=ds.SharedFunctionalGroupsSequence[0].PixelMeasuresSequence[0].PixelSpacing\n",
    "        volume = slices[0].pixel_array.astype(np.float32)\n",
    "    else:\n",
    "    \n",
    "        posz_list=[]\n",
    "        for s in slices:\n",
    "            # Plane position sequence\n",
    "            pos = s.ImagePositionPatient[2]\n",
    "            \n",
    "            posz_list.append(pos)      \n",
    "    \n",
    "    \n",
    "        dy,dx=slices[0].PixelSpacing\n",
    "        slices.sort(key=lambda s: float(s.ImagePositionPatient[2]))\n",
    "\n",
    "        \n",
    "        volume_list = []\n",
    "        for s in slices:\n",
    "            image = s.pixel_array.astype(np.float32)\n",
    "            volume_list.append(image)#.astype(np.uint8))  # 轉 uint8 節省空間\n",
    "    \n",
    "        # 組成 3D array (z, y, x)\n",
    "        volume = np.stack(volume_list, axis=0)\n",
    "    \n",
    "    \n",
    "    vol=volume#np.load(f\"input/npy/{sid}.npy\")\n",
    "    vol_t = torch.from_numpy(vol).to(dtype=torch.float32)[None, None, ...]  # [N=1, C=1, Z, Y, X]\n",
    "    \n",
    "    \n",
    "    dz=np.ptp(posz_list)/(vol.shape[0]-1)\n",
    "    in_sz = np.array(vol.shape, float)       # [Z, Y, X]\n",
    "    in_sp = np.array([dz,dy,dx], float)   # [sz, sy, sx]\n",
    "    out_sp = np.array([0.7, 0.7, 0.7], float)\n",
    "    \n",
    "    out_sz = (in_sz * (in_sp / out_sp)).round().astype(int)\n",
    "    out_sz = tuple(int(x) for x in out_sz)   # (Z, Y, X)\n",
    "    \n",
    "    # grid_sample 需要目標 size（注意順序 D,H,W）\n",
    "    vol_resampled = F.interpolate(\n",
    "        vol_t, size=out_sz, mode=\"trilinear\", align_corners=False\n",
    "    )  # [1, 1, Z, Y, X]\n",
    "    \n",
    "    img = vol_resampled[0, 0]  # [Z, Y, X]\n",
    "    if img.shape[0]<100:\n",
    "        return None\n",
    "    \n",
    "\n",
    "    img = percentile_clip_minmax_np(img, pmin=0.5, pmax=99.5)\n",
    "\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edc80bf-390f-4c3c-b350-cbab38df7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predictor_highest(x):\n",
    "    out = model(x)\n",
    "    return out[0] if isinstance(out, (list, tuple)) else out\n",
    "\n",
    "\n",
    "def percentile_clip_minmax_np(img: torch.Tensor, pmin=1.0, pmax=99.0):\n",
    "    # 保證是 float32，放到 CPU\n",
    "    arr = img.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # 算分位數\n",
    "    low = np.percentile(arr, pmin)\n",
    "    high = np.percentile(arr, pmax)\n",
    "\n",
    "    # clip + min-max normalize\n",
    "    arr = np.clip(arr, low, high)\n",
    "    arr = (arr - low) / (high - low + 1e-8)\n",
    "\n",
    "    # 回 torch.float32，放回原本裝置\n",
    "    return torch.from_numpy(arr).to(img.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45baa92c-b4e8-4ad0-911f-8d3e78143220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_seg_pred(img,save_dir,sid):\n",
    "    os.makedirs(save_dir,exist_ok=True)\n",
    "    if img.ndim == 3:\n",
    "        img = img[None, ...]  # -> [C,H,W,D]\n",
    "        \n",
    "    sample = {\"image\": img}\n",
    "    sample = val_tf(sample)\n",
    "    \n",
    "    img = sample[\"image\"].to(device)  \n",
    "    # === sliding-window 推論 ===\n",
    "    with torch.no_grad():\n",
    "        logits = sliding_window_inference(\n",
    "            img.unsqueeze(0), roi_size=ROI_SIZE, sw_batch_size=2,\n",
    "            predictor=predictor_highest, overlap=0.5, mode=\"gaussian\"\n",
    "        )\n",
    "        prob = torch.sigmoid(logits)\n",
    "    pred=(prob>0.5).squeeze().cpu().numpy().astype(int)\n",
    "    \n",
    "    labels, N = cc3d.connected_components(pred, connectivity=26, return_N=True)\n",
    "    # if N == 0:\n",
    "    #     return None, bin_\n",
    "    \n",
    "    counts = np.bincount(labels.ravel())\n",
    "    keep_ids = np.where(counts >= 800)[0]; keep_ids = keep_ids[keep_ids != 0]\n",
    "    pred = np.isin(labels, keep_ids).astype(np.uint8)\n",
    "    \n",
    "    np.save(f\"{save_dir}/{sid}.npy\",pred)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resize_volume_3d(volume,target_shape=(64,448,448)):\n",
    "    target_depth, target_height, target_width = target_shape[0], target_shape[1],target_shape[2]\n",
    "    current_shape = volume.shape\n",
    "    target_shape = (target_depth, target_height, target_width)\n",
    "    if current_shape == target_shape:\n",
    "        return volume\n",
    "    zoom_factors = [\n",
    "        target_shape[i] / current_shape[i] for i in range(3)\n",
    "    ]\n",
    "    resized_volume = ndimage.zoom(volume, zoom_factors, order=1, mode='nearest')\n",
    "    resized_volume = resized_volume[:target_depth, :target_height, :target_width]\n",
    "    pad_width = [\n",
    "        (0, max(0, target_depth - resized_volume.shape[0])),\n",
    "        (0, max(0, target_height - resized_volume.shape[1])),\n",
    "        (0, max(0, target_width - resized_volume.shape[2]))\n",
    "    ]\n",
    "    if any(pw[1] > 0 for pw in pad_width):\n",
    "        resized_volume = np.pad(resized_volume, pad_width, mode='edge')\n",
    "    return resized_volume.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730c785f-01f7-4057-9b03-f88863219854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z_df=axis_df[axis_df[\"axis\"]==\"z\"].reset_index(drop=True)\n",
    "#z_df=z_df.merge(df[[\"SeriesInstanceUID\",\"Modality\"]],how=\"left\",on=\"SeriesInstanceUID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838659cb-8010-478c-b342-15d5af8416ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d94ba12a64049ea8802d26787ef605d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fateplsf/miniconda3/envs/py3.10/lib/python3.10/site-packages/monai/inferers/utils.py:226: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
      "  win_data = torch.cat([inputs[win_slice] for win_slice in unravel_slice]).to(sw_device)\n",
      "/home/fateplsf/miniconda3/envs/py3.10/lib/python3.10/site-packages/monai/inferers/utils.py:370: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:306.)\n",
      "  out[idx_zm] += p\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sid_list=[]\n",
    "minz_range=[]\n",
    "maxz_range=[]\n",
    "for sid in tqdm(z_df[\"SeriesInstanceUID\"].values):\n",
    "    # volume=get_volume(sid)\n",
    "    # save_seg_pred(volume,save_dir,sid)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        volume=get_volume(sid)\n",
    "        seg_pred=save_seg_pred(volume,save_dir,sid)\n",
    "        \n",
    "        resized_v = resize_volume_3d(seg_pred,target_shape=(64,448,448))\n",
    "        \n",
    "        np.save(f\"{pred_resize_dir}/{sid}.npy\",resized_v)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc07398-0a25-4940-8d44-b59fd583e23d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
